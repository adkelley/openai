import gleam/dict.{type Dict}
import gleam/option.{type Option}

pub type Response {
  Response(
    /// Whether to run the model response in the background.
    background: Bool,
    /// Unique identifier for this Response.
    id: String,
    /// The object type of this resource - always set to response.
    object: String,
    /// Unix timestamp (in seconds) of when this Response was created.
    created_at: Int,
    // /// The status of the response generation. One of completed, failed, in_progress, cancelled, queued, or incomplete.
    status: String,
    /// ???
    billing: Billing,
    // An error object returned when the model fails to generate a Response.
    error: Option(Error),
    /// Details about why the response is incomplete.
    incomplete_details: Option(IncompleteDetails),
    /// A system (or developer) message inserted into the model's context. When using along with previous_response_id, the
    /// instructions from a previous response will not be carried over to the next response. This makes it simple to swap out
    /// system (or developer) messages in new responses.
    instructions: Option(Instructions),
    /// An upper bound for the number of tokens that can be generated for a response, including visible output
    /// tokens and reasoning tokens.
    max_output_tokens: Option(Int),
    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number
    /// applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    max_tool_calls: Option(Int),
    /// Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different
    /// capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
    model: String,
    /// An array of content items generated by the model. The length and order of items in the output array is dependent on the
    /// model's response. Rather than accessing the first item in the output array and assuming it's an assistant message with
    /// the content generated by the model, you might consider using the output_text property where supported in SDKs.
    output: List(Output),
    /// Whether to allow the model to run tool calls in parallel.
    parallel_tool_calls: Bool,
    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about
    /// conversation state. Cannot be used in conjunction with conversation.
    previous_response_id: Option(String),
    /// Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the user field. Learn more.
    prompt_cache_key: Option(String),
    reasoning: Reasoning,
    /// A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should
    /// be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending
    /// us any identifying information. Learn more.
    safety_identifier: Option(String),
    // If set to 'default', then the request will be processed with the standard pricing and performance for the selected model.
    // If set to 'flex' or 'priority', then the request will be processed with the corresponding service tier.
    // When not set, the default behavior is 'auto'.
    /// Specifies the processing type used for serving the request.
    /// If set to 'auto', then the request will be processed with the service tier configured in the Project settings.
    /// Unless otherwise configured, the Project will use 'default'.
    service_tier: String,
    /// ???
    store: Bool,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
    /// lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
    temperature: Float,
    /// Configuration options for a text response from the model. Can be plain text or structured JSON data.
    text: Text,
    // TODO support the tool choice object (e.g., Allowed tools)
    // 1. none means the model will not call any tool and instead generates a message.
    // 2. auto means the model can pick between generating a message or calling one or more tools.
    // 3. required means the model must call one or more tools.
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see
    /// how to specify which tools the model can call.
    tool_choice: String,
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how
    /// to specify which tools the model can call.
    tools: List(Tool),
    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
    /// each with an associated log probability.
    top_logprobs: Int,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens
    /// with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    /// gpt-5 and o-series models only
    top_p: Float,
    /// The truncation strategy to use for the model response.
    /// 1. auto: If the input to this Response exceeds the model's context window size, the model will truncate the response to fit the
    /// context window by dropping items from the beginning of the conversation.
    /// 2. disabled (default): If the input size will exceed the context window size for a model, the request will fail with a 400 error.
    truncation: String,
    /// Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
    usage: Usage,
    /// DEPRECATED
    /// This field is being replaced by safety_identifier and prompt_cache_key. Use prompt_cache_key instead to maintain caching
    /// optimizations. A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests
    /// and to help OpenAI detect and prevent abuse. Learn more.
    user: Option(String),
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the
    /// object in a structured format, and querying for objects via API or the dashboard.
    /// Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
    metadata: Dict(String, String),
  )
}

pub type Billing {
  Billing(payer: String)
}

pub type Error {
  Error(code: String, message: String)
}

pub type IncompleteDetails {
  IncompleteDetails(
    /// The reason why the response is incomplete.
    reason: String,
  )
}

// TODO: add Input Item List
pub type Instructions {
  /// A text input to the model, equivalent to a text input with the developer role.
  InstructionsText(String)
}

// This is the type key
pub type Output {
  Message(
    content: List(OutputContent),
    id: String,
    role: String,
    status: String,
  )
  WebSearchCall(action: Action, id: String, status: String)
}

pub type OutputContent {
  OutputText(annotations: List(Annotation), text: String)
}

pub type Action {
  SearchAction(
    /// The search query.
    query: String,
    /// The sources used in the search.
    sources: Sources,
  )
  OpenPageAction(
    /// The action type.
    /// The URL opened by the model.
    url: String,
  )
  FindAction(
    /// The pattern or text to search for within the page.
    pattern: String,
    /// The URL opened by the model.
    url: String,
  )
}

// pub fn action_decoder() -> Decoder(Action) {
//   let search_decoder = fn() {
//     use query <- decode.field("query", decode.string)
//     use sources <- decode.optional_field(
//       "sources",
//       Sources(url: "n/a"),
//       sources_decoder(),
//     )
//     decode.success(SearchAction(query:, sources:))
//   }
//   use type_ <- decode.field("type", decode.string)
//   case type_ {
//     "search" -> search_decoder()
//     _ -> {
//       echo "action_decoder paniced"
//       panic
//     }
//   }
// }

pub type Sources {
  Sources(
    /// The URL of the source.
    url: String,
  )
}

// pub fn sources_decoder() -> Decoder(Sources) {
//   // The type of source. Always url.
//   use type_ <- decode.field("type", decode.string)
//   // TODO Error or panic?
//   assert type_ == "url"
//   use url <- decode.field("url", decode.string)
//   decode.success(Sources(url:))
// }

pub type Annotation {
  /// A citation to a file.
  FileCitation(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
  /// A citation for a web resource used to generate a model response.
  URLCitation(
    /// The index of the last character of the URL citation in the message.
    end_index: Int,
    /// The index of the first character of the URL citation in the message.
    start_index: Int,
    /// The title of the web resource.
    title: String,
    /// The URL of the web resource.
    url: String,
  )
  /// A path to a file.
  FilePath(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
}

// fn file_citation_decoder() {
//   use file_id <- decode.field("file_id", decode.string)
//   use index <- decode.field("index", decode.int)
//   decode.success(FileCitation(file_id:, index:))
// }

// fn url_citation_decoder() {
//   use end_index <- decode.field("end_index", decode.int)
//   use start_index <- decode.field("start_index", decode.int)
//   use title <- decode.field("title", decode.string)
//   use url <- decode.field("url", decode.string)
//   decode.success(URLCitation(end_index:, start_index:, title:, url:))
// }

// fn container_file_citation_decoder() {
//   todo
// }

// fn file_path_decoder() {
//   use file_id <- decode.field("file_id", decode.string)
//   use index <- decode.field("index", decode.int)
//   decode.success(FilePath(file_id:, index:))
// }

// pub fn annotation_decoder() {
//   use type_ <- decode.field("type", decode.string)
//   case type_ {
//     "file_citation" -> file_citation_decoder()
//     "url_citation" -> url_citation_decoder()
//     "container_file_citation" -> container_file_citation_decoder()
//     "file_path" -> file_path_decoder()
//     _ -> panic
//   }
// }

pub type Reasoning {
  Reasoning(
    /// Constrains effort on reasoning for reasoning models. Currently supported values are minimal, low, medium,
    /// and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
    effort: Option(String),
    /// A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's
    /// reasoning process. One of auto, concise, or detailed.
    summary: Option(String),
  )
}

pub type Text {
  Text(format: Format, verbosity: String)
}

pub type Format {
  Format(type_: String)
}

pub type Tool {
  /// Defines a function in your own code the model can choose to call. Learn more about [function calling](https://platform.openai.com/docs/guides/function-calling).
  Function(
    /// The name of the function to call.
    name: String,
    /// A JSON schema object describing the parameters of the function.
    /// TODO: Should this be a JSON type?
    parameters: String,
    /// Whether to enforce strict parameter validation.
    strict: Bool,
    /// A description of the function. Used by the model to determine whether or not to call the function.
    description: Option(String),
  )
  /// A tool that searches for relevant content from uploaded files. Learn more about the [file search tool](https://platform.openai.com/docs/guides/tools-file-search).
  FileSearch(
    /// The IDs of the vector stores to search.
    vector_store_ids: List(String),
    /// A filter to apply based on file attributes.
    filters: FileSearchFilters,
    /// The maximum number of results to return. This number should be between 1 and 50 inclusive.
    max_num_results: Int,
    /// Ranking options for search.
    ranking_options: RankingOptions,
  )
  ComputerUse(
    /// The height of the computer display.
    display_height: Int,
    /// The width of the computer display.
    display_width: Int,
    /// The type of computer environment to control.
    environment: String,
  )
  WebSearch(
    /// High level guidance for the amount of context window space to use for the search.
    search_context_size: Option(String),
    /// Approximate location parameters for the search.
    user_location: Option(UserLocation),
    /// Filters for the search.
    filters: Option(Filters),
  )
}

// pub fn tool_decoder() -> Decoder(Tool) {
//   decode.one_of(function_decoder(), or: [
//     file_search_decoder(),
//     computer_use_decoder(),
//     web_search_decoder(),
//   ])
// }

// fn computer_use_decoder() -> Decoder(Tool) {
//   use display_height <- decode.field("display_height", decode.int)
//   use display_width <- decode.field("display_width", decode.int)
//   use environment <- decode.field("environment", decode.string)
//   decode.success(ComputerUse(display_height:, display_width:, environment:))
// }

// fn file_search_decoder() -> Decoder(Tool) {
//   use vector_store_ids <- decode.field(
//     "vector_store_ids",
//     decode.list(decode.string),
//   )
//   use filters <- decode.field("filters", file_search_filter_decoder())
//   use max_num_results <- decode.field("max_num_results", decode.int)
//   use ranking_options <- decode.field(
//     "ranking_options",
//     ranking_options_decoder(),
//   )
//   decode.success(FileSearch(
//     vector_store_ids:,
//     filters:,
//     max_num_results:,
//     ranking_options:,
//   ))
// }

pub type FileSearchFilters {
  ComparisonFilter(
    /// The key to compare against the value.
    key: String,
    /// Specifies the comparison operator: eq, ne, gt, gte, lt, lte, in, nin.
    type_: String,
    /// The value to compare against the attribute key; supports string, number, or boolean types.
    value: Value,
  )
}

// fn file_search_filter_decoder() -> Decoder(FileSearchFilters) {
//   decode.one_of(comparison_filter_decoder(), [])
// }

// fn comparison_filter_decoder() -> Decoder(FileSearchFilters) {
//   use key <- decode.field("key", decode.string)
//   use type_ <- decode.field("type", decode.string)
//   use value <- decode.field("value", value_decoder())
//   decode.success(ComparisonFilter(key:, type_:, value:))
// }

pub type Value {
  ValueString(String)
  ValueFloat(Float)
  ValueBool(Bool)
  ValueArrayString(List(String))
  ValueArrayFloat(List(Float))
  ValueArrayBool(List(Bool))
}

// fn value_decoder() -> Decoder(Value) {
//   let value_string_decoder = fn() {
//     use value_string <- decode.field("value", decode.string)
//     decode.success(ValueString(value_string))
//   }
//   let value_float_decoder = fn() {
//     use value_float <- decode.field("value", decode.float)
//     decode.success(ValueFloat(value_float))
//   }
//   let value_bool_decoder = fn() {
//     use value_bool <- decode.field("value", decode.bool)
//     decode.success(ValueBool(value_bool))
//   }
//   let value_array_string_decoder = fn() {
//     use value_array <- decode.field("value", decode.list(decode.string))
//     decode.success(ValueArrayString(value_array))
//   }
//   let value_array_float_decoder = fn() {
//     use value_array_float <- decode.field("value", decode.list(decode.float))
//     decode.success(ValueArrayFloat(value_array_float))
//   }
//   let value_array_bool_decoder = fn() {
//     use value_array_bool <- decode.field("value", decode.list(decode.bool))
//     decode.success(ValueArrayBool(value_array_bool))
//   }

//   decode.one_of(value_string_decoder(), [
//     value_float_decoder(),
//     value_bool_decoder(),
//     value_array_string_decoder(),
//     value_array_float_decoder(),
//     value_array_bool_decoder(),
//   ])
// }

pub type RankingOptions {
  RankingOptions(ranker: String, score_threshold: Float)
}

// fn ranking_options_decoder() -> Decoder(RankingOptions) {
//   use ranker <- decode.field("ranker", decode.string)
//   use score_threshold <- decode.field("score_threshold", decode.float)
//   decode.success(RankingOptions(ranker:, score_threshold:))
// }

// fn function_decoder() {
//   use name <- decode.field("name", decode.string)
//   use parameters <- decode.field("parameters", decode.string)
//   use strict <- decode.field("strict", decode.bool)
//   use description <- decode.field("description", decode.optional(decode.string))
//   decode.success(Function(name:, parameters:, strict:, description:))
// }

// fn file_search_filter_decoder() {
//   todo
// }

// fn ranking_options_decoder() {
//   decode.string
// }

// fn file_search_decoder() {
//   use _type_ <- decode.field("type", decode.string)
//   use vector_store_ids <- decode.field(
//     "vector_store_ids",
//     decode.list(decode.string),
//   )
//   use filters <- decode.field("filters", file_search_filter_decoder())
//   use max_num_results <- decode.field("max_num_results", decode.int)
//   use ranking_options <- decode.field(
//     "ranking_options",
//     ranking_options_decoder(),
//   )
//   decode.success(response.FileSearch(
//     vector_store_ids:,
//     filters:,
//     max_num_results:,
//     ranking_options:,
//   ))
// }

// fn environment_decoder() {
//   decode.string
// }

// fn computer_use_decoder() {
//   use display_height <- decode.field("display_height", decode.int)
//   use display_width <- decode.field("display_width", decode.int)
//   use environment <- decode.field("environment", environment_decoder())
//   decode.success(response.ComputerUse(
//     display_height:,
//     display_width:,
//     environment:,
//   ))
// }

// fn search_context_size_decoder() {
//   decode.string
// }

// fn user_location_decoder() {
//   decode.string
// }

// pub fn web_search_decoder() {
//   use type_ <- decode.field("type", decode.string)
//   assert type_ == "web_search"
//   use search_context_size <- decode.field(
//     "search_context_size",
//     decode.optional(decode.string),
//   )
//   use filters <- decode.field("filters", decode.optional(filters_decoder()))
//   use user_location <- decode.field(
//     "user_location",
//     decode.optional(user_location_decoder()),
//   )
//   decode.success(WebSearch(search_context_size:, filters:, user_location:))
// }

// \"tools\": [\n    {\n      \"type\": \"web_search\",\n      \"filters\": null,\n      \"search_context_size\": \"medium\",\n      \"user_location\": {\n        \"type\": \"approximate\",\n        \"city\": null,\n        \"country\": \"US\",\n        \"region\": null,\n        \"timezone\": null\n      }\n    }\n  ],\n

pub type Filters {
  Filters(allowed_domains: Option(List(String)))
}

// fn filters_decoder() -> Decoder(Filters) {
//   use allowed_domains <- decode.field(
//     "allowed_domains",
//     decode.optional(decode.list(decode.string)),
//   )
//   decode.success(Filters(allowed_domains:))
// }

pub type UserLocation {
  UserLocation(
    /// Free text input for the city of the user, e.g. San Francisco.
    city: Option(String),
    /// The two-letter ISO country code of the user, e.g. US.
    country: Option(String),
    /// Free text input for the region of the user, e.g. California.
    region: Option(String),
    /// The IANA timezone of the user, e.g. America/Los_Angeles.
    timezone: Option(String),
    /// The type of location approximation. Always approximate.
    type_: Option(String),
  )
}

// fn user_location_decoder() -> Decoder(UserLocation) {
//   use city <- decode.field("city", decode.optional(decode.string))
//   use country <- decode.field("country", decode.optional(decode.string))
//   use region <- decode.field("region", decode.optional(decode.string))
//   use timezone <- decode.field("timezone", decode.optional(decode.string))
//   use type_ <- decode.field("type", decode.optional(decode.string))
//   decode.success(UserLocation(city:, country:, region:, timezone:, type_:))
// }

pub type Usage {
  Usage(
    input_tokens: Int,
    input_tokens_details: InputTokensDetails,
    output_tokens: Int,
    output_tokens_details: OutputTokensDetails,
    total_tokens: Int,
  )
}

// pub fn usage_decoder() {
//   let input_tokens_details_decoder = fn() {
//     use cached_tokens <- decode.field("cached_tokens", decode.int)
//     decode.success(InputTokensDetails(cached_tokens:))
//   }
//   let output_tokens_details_decoder = fn() {
//     use reasoning_tokens <- decode.field("reasoning_tokens", decode.int)
//     decode.success(OutputTokensDetails(reasoning_tokens:))
//   }
//   use input_tokens <- decode.field("input_tokens", decode.int)
//   use input_tokens_details <- decode.field(
//     "input_tokens_details",
//     input_tokens_details_decoder(),
//   )
//   use output_tokens <- decode.field("output_tokens", decode.int)
//   use output_tokens_details <- decode.field(
//     "output_tokens_details",
//     output_tokens_details_decoder(),
//   )
//   use total_tokens <- decode.field("total_tokens", decode.int)
//   decode.success(Usage(
//     input_tokens:,
//     input_tokens_details:,
//     output_tokens:,
//     output_tokens_details:,
//     total_tokens:,
//   ))
// }

pub type InputTokensDetails {
  InputTokensDetails(cached_tokens: Int)
}

pub type OutputTokensDetails {
  OutputTokensDetails(reasoning_tokens: Int)
}
