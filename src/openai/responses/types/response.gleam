import gleam/dict.{type Dict}
import gleam/dynamic
import gleam/option.{type Option}

pub type Response {
  Response(
    /// Whether to run the model response in the background.
    background: Bool,
    /// Unique identifier for this Response.
    id: String,
    /// The object type of this resource - always set to response.
    object: String,
    /// Unix timestamp (in seconds) of when this Response was created.
    created_at: Int,
    // /// The status of the response generation. One of completed, failed, in_progress, cancelled, queued, or incomplete.
    status: String,
    /// ???
    billing: Billing,
    // An error object returned when the model fails to generate a Response.
    error: Option(Error),
    /// Details about why the response is incomplete.
    incomplete_details: Option(IncompleteDetails),
    /// A system (or developer) message inserted into the model's context. When using along with previous_response_id, the
    /// instructions from a previous response will not be carried over to the next response. This makes it simple to swap out
    /// system (or developer) messages in new responses.
    instructions: Option(Instructions),
    /// An upper bound for the number of tokens that can be generated for a response, including visible output
    /// tokens and reasoning tokens.
    max_output_tokens: Option(Int),
    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number
    /// applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    max_tool_calls: Option(Int),
    /// Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different
    /// capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
    model: String,
    /// An array of content items generated by the model. The length and order of items in the output array is dependent on the
    /// model's response. Rather than accessing the first item in the output array and assuming it's an assistant message with
    /// the content generated by the model, you might consider using the output_text property where supported in SDKs.
    output: List(Output),
    /// Whether to allow the model to run tool calls in parallel.
    parallel_tool_calls: Bool,
    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about
    /// conversation state. Cannot be used in conjunction with conversation.
    previous_response_id: Option(String),
    /// Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the user field. Learn more.
    prompt_cache_key: Option(String),
    reasoning: Reasoning,
    /// A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should
    /// be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending
    /// us any identifying information. Learn more.
    safety_identifier: Option(String),
    // If set to 'default', then the request will be processed with the standard pricing and performance for the selected model.
    // If set to 'flex' or 'priority', then the request will be processed with the corresponding service tier.
    // When not set, the default behavior is 'auto'.
    /// Specifies the processing type used for serving the request.
    /// If set to 'auto', then the request will be processed with the service tier configured in the Project settings.
    /// Unless otherwise configured, the Project will use 'default'.
    service_tier: String,
    /// ???
    store: Bool,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
    /// lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
    temperature: Float,
    /// Configuration options for a text response from the model. Can be plain text or structured JSON data.
    text: Text,
    // TODO support the tool choice object (e.g., Allowed tools)
    // 1. none means the model will not call any tool and instead generates a message.
    // 2. auto means the model can pick between generating a message or calling one or more tools.
    // 3. required means the model must call one or more tools.
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see
    /// how to specify which tools the model can call.
    tool_choice: String,
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how
    /// to specify which tools the model can call.
    tools: List(Tool),
    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
    /// each with an associated log probability.
    top_logprobs: Int,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens
    /// with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    /// gpt-5 and o-series models only
    top_p: Float,
    /// The truncation strategy to use for the model response.
    /// 1. auto: If the input to this Response exceeds the model's context window size, the model will truncate the response to fit the
    /// context window by dropping items from the beginning of the conversation.
    /// 2. disabled (default): If the input size will exceed the context window size for a model, the request will fail with a 400 error.
    truncation: String,
    /// Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
    usage: Usage,
    /// DEPRECATED
    /// This field is being replaced by safety_identifier and prompt_cache_key. Use prompt_cache_key instead to maintain caching
    /// optimizations. A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests
    /// and to help OpenAI detect and prevent abuse. Learn more.
    user: Option(String),
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the
    /// object in a structured format, and querying for objects via API or the dashboard.
    /// Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
    metadata: Dict(String, String),
  )
}

pub type Billing {
  Billing(payer: String)
}

pub type Error {
  Error(code: String, message: String)
}

pub type IncompleteDetails {
  IncompleteDetails(
    /// The reason why the response is incomplete.
    reason: String,
  )
}

// TODO: add Input Item List
pub type Instructions {
  /// A text input to the model, equivalent to a text input with the developer role.
  InstructionsText(String)
}

// This is the type key
pub type Output {
  OutputMessage(
    content: List(OutputMessageContent),
    id: String,
    role: String,
    status: String,
  )
  OutputWebSearchCall(action: Action, id: String, status: String)
  OutputMcpListTools(
    id: String,
    server_label: String,
    tools: List(OutputMcpListTools),
  )
  OutputReasoning(
    /// The unique identifier of the reasoning content.
    id: String,
    /// Reasoning summary content.
    summary: List(OutputReasoningSummary),
    /// Reasoning text content.
    content: List(OutputReasoningContent),
  )
  OutputMcpCall(
    id: String,
    status: String,
    approval_request_id: Option(String),
    arguments: String,
    error: Option(OutputMcpCallError),
    name: String,
    output: String,
    server_label: String,
  )
  OutputMcpApprovalRequest(
    id: String,
    arguments: String,
    name: String,
    server_label: String,
  )
  OutputFunctionCall(
    status: String,
    id: String,
    call_id: String,
    /// The name of the function to call.
    name: String,
    /// Function specifec Json to parse within the caller app
    arguments: String,
  )
  // type "shell_call"
  OutputShellCall(
    id: String,
    call_id: String,
    action: OutputShellCallAction,
    status: String,
    environment: Option(String),
  )
}

pub type OutputShellCallAction {
  OutputShellCallAction(
    commands: List(String),
    timeout_ms: Int,
    max_output_length: Int,
  )
}

pub type OutputMcpCallError {
  OutputMcpCallError(code: Int, message: String)
}

/// Reasoning text content.
pub type OutputReasoningSummary {
  OutputReasoningSummary(text: String)
}

pub type OutputReasoningContent {
  OutputReasoningContent(text: String)
}

pub type OutputMcpListTools {
  OutputMcpToolItem(
    // TODO How to parse annotations
    // annotations: Option(List(Annotation)),
    description: String,
    // TODO ponder parsing input schema
    // input_schema: InputSchema,
    name: String,
  )
}

pub type OutputMessageContent {
  OutputText(annotations: List(Annotation), text: String)
}

pub type OutputTextContent {
  OutputTextContent(annotations: List(Annotation), text: String)
}

pub type Action {
  SearchAction(
    /// The search query.
    query: String,
    /// The sources used in the search.
    sources: Sources,
  )
  OpenPageAction(
    /// The action type.
    /// The URL opened by the model.
    url: String,
  )
  FindAction(
    /// The pattern or text to search for within the page.
    pattern: String,
    /// The URL opened by the model.
    url: String,
  )
}

// pub fn action_decoder() -> Decoder(Action) {
//   let search_decoder = fn() {
//     use query <- decode.field("query", decode.string)
//     use sources <- decode.optional_field(
//       "sources",
//       Sources(url: "n/a"),
//       sources_decoder(),
//     )
//     decode.success(SearchAction(query:, sources:))
//   }
//   use type_ <- decode.field("type", decode.string)
//   case type_ {
//     "search" -> search_decoder()
//     _ -> {
//       echo "action_decoder paniced"
//       panic
//     }
//   }
// }

pub type Sources {
  Sources(
    /// The URL of the source.
    url: String,
  )
}

pub type Annotation {
  /// A citation to a file.
  FileCitation(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
  /// A citation for a web resource used to generate a model response.
  URLCitation(
    /// The index of the last character of the URL citation in the message.
    end_index: Int,
    /// The index of the first character of the URL citation in the message.
    start_index: Int,
    /// The title of the web resource.
    title: String,
    /// The URL of the web resource.
    url: String,
  )
  /// A path to a file.
  FilePath(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
  /// A citation for a container file used to generate a model response.
  ContainerFileCitation(
    /// The ID of the container file.
    container_id: String,
    /// The index of the last character of the container file citation in the message.
    end_index: Int,
    /// The ID of the file.
    file_id: String,
    /// The filename of the container file cited.
    filename: String,
    /// The index of the first character of the container file citation in the message.
    start_index: Int,
  )
}

pub type Reasoning {
  Reasoning(
    /// Constrains effort on reasoning for reasoning models. Currently supported values are minimal, low, medium,
    /// and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
    effort: Option(String),
    /// A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's
    /// reasoning process. One of auto, concise, or detailed.
    summary: Option(String),
  )
}

pub type Text {
  Text(format: Format, verbosity: String)
}

pub type Format {
  Format(type_: String)
}

pub type Tool {
  /// Defines a function in your own code the model can choose to call. Learn
  /// more about [function calling](https://platform.openai.com/docs/guides/
  /// function-calling).
  Function(
    /// The function's name (e.g. get_weather)
    name: String,
    /// Details on when and how to use the function
    description: String,
    /// JSON schema defining the function's input arguments
    parameters: dynamic.Dynamic,
    /// Whether to enforce strict mode for the function call
    strict: Bool,
  )
  /// A tool that searches for relevant content from uploaded files. Learn more
  /// about the [file search tool](https://platform.openai.com/docs/guides/
  /// tools-file-search).
  FileSearch(
    /// The IDs of the vector stores to search.
    vector_store_ids: List(String),
    /// A filter to apply based on file attributes.
    filters: FileSearchFilters,
    /// The maximum number of results to return. This number should be between 1 and 50 inclusive.
    max_num_results: Int,
    /// Ranking options for search.
    ranking_options: RankingOptions,
  )
  ComputerUse(
    /// The height of the computer display.
    display_height: Int,
    /// The width of the computer display.
    display_width: Int,
    /// The type of computer environment to control.
    environment: String,
  )
  WebSearch(
    /// High level guidance for the amount of context window space to use for the search.
    search_context_size: Option(String),
    /// Approximate location parameters for the search.
    user_location: Option(UserLocation),
    /// Filters for the search.
    filters: Option(Filters),
  )
  Mcp(
    allowed_tools: Option(List(String)),
    // TODO parse headers?
    // headers: Option(List(#(String, String))),
    require_approval: ToolMcpRequireApproval,
    server_description: Option(String),
    server_url: String,
    server_label: String,
  )
  // TODO Document these args
  // type: "shell"
  Shell
}

pub type ToolMcpRequireApproval {
  ToolMcpRequireApproval(always: Option(Always), never: Option(Never))
}

pub type Always {
  Always(read_only: Option(Bool), tool_names: List(String))
}

pub type Never {
  Never(read_only: Option(Bool), tool_names: List(String))
}

pub type FileSearchFilters {
  ComparisonFilter(
    /// The key to compare against the value.
    key: String,
    /// Specifies the comparison operator: eq, ne, gt, gte, lt, lte, in, nin.
    type_: String,
    /// The value to compare against the attribute key; supports string, number, or boolean types.
    value: Value,
  )
}

pub type Value {
  ValueString(String)
  ValueFloat(Float)
  ValueBool(Bool)
  ValueArrayString(List(String))
  ValueArrayFloat(List(Float))
  ValueArrayBool(List(Bool))
}

pub type RankingOptions {
  RankingOptions(ranker: String, score_threshold: Float)
}

pub type Filters {
  Filters(allowed_domains: Option(List(String)))
}

// fn filters_decoder() -> Decoder(Filters) {
//   use allowed_domains <- decode.field(
//     "allowed_domains",
//     decode.optional(decode.list(decode.string)),
//   )
//   decode.success(Filters(allowed_domains:))
// }

pub type UserLocation {
  UserLocation(
    /// Free text input for the city of the user, e.g. San Francisco.
    city: Option(String),
    /// The two-letter ISO country code of the user, e.g. US.
    country: Option(String),
    /// Free text input for the region of the user, e.g. California.
    region: Option(String),
    /// The IANA timezone of the user, e.g. America/Los_Angeles.
    timezone: Option(String),
    /// The type of location approximation. Always approximate.
    type_: Option(String),
  )
}

pub type Usage {
  Usage(
    input_tokens: Int,
    input_tokens_details: InputTokensDetails,
    output_tokens: Int,
    output_tokens_details: OutputTokensDetails,
    total_tokens: Int,
  )
}

pub type InputTokensDetails {
  InputTokensDetails(cached_tokens: Int)
}

pub type OutputTokensDetails {
  OutputTokensDetails(reasoning_tokens: Int)
}
