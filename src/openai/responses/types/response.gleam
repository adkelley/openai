import gleam/dict.{type Dict}
import gleam/dynamic
import gleam/option.{type Option}

pub type Response {
  Response(
    /// Whether to run the model response in the background.
    background: Bool,
    /// Unique identifier for this Response.
    id: String,
    /// The object type of this resource - always set to response.
    object: String,
    /// Unix timestamp (in seconds) of when this Response was created.
    created_at: Int,
    // /// The status of the response generation. One of completed, failed, in_progress, cancelled, queued, or incomplete.
    status: String,
    /// ???
    billing: Billing,
    // An error object returned when the model fails to generate a Response.
    error: Option(Error),
    /// Details about why the response is incomplete.
    incomplete_details: Option(IncompleteDetails),
    /// A system (or developer) message inserted into the model's context. When using along with previous_response_id, the
    /// instructions from a previous response will not be carried over to the next response. This makes it simple to swap out
    /// system (or developer) messages in new responses.
    instructions: Option(Instructions),
    /// An upper bound for the number of tokens that can be generated for a response, including visible output
    /// tokens and reasoning tokens.
    max_output_tokens: Option(Int),
    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number
    /// applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    max_tool_calls: Option(Int),
    /// Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different
    /// capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
    model: String,
    /// An array of content items generated by the model. The length and order of items in the output array is dependent on the
    /// model's response. Rather than accessing the first item in the output array and assuming it's an assistant message with
    /// the content generated by the model, you might consider using the output_text property where supported in SDKs.
    output: List(Output),
    /// Whether to allow the model to run tool calls in parallel.
    parallel_tool_calls: Bool,
    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about
    /// conversation state. Cannot be used in conjunction with conversation.
    previous_response_id: Option(String),
    /// Used by OpenAI to cache responses for similar requests to optimize your cache hit rates. Replaces the user field. Learn more.
    prompt_cache_key: Option(String),
    reasoning: Reasoning,
    /// A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies. The IDs should
    /// be a string that uniquely identifies each user. We recommend hashing their username or email address, in order to avoid sending
    /// us any identifying information. Learn more.
    safety_identifier: Option(String),
    // If set to 'default', then the request will be processed with the standard pricing and performance for the selected model.
    // If set to 'flex' or 'priority', then the request will be processed with the corresponding service tier.
    // When not set, the default behavior is 'auto'.
    /// Specifies the processing type used for serving the request.
    /// If set to 'auto', then the request will be processed with the service tier configured in the Project settings.
    /// Unless otherwise configured, the Project will use 'default'.
    service_tier: String,
    /// ???
    store: Bool,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while
    /// lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
    temperature: Float,
    /// Configuration options for a text response from the model. Can be plain text or structured JSON data.
    text: Text,
    // TODO support the tool choice object (e.g., Allowed tools)
    // 1. none means the model will not call any tool and instead generates a message.
    // 2. auto means the model can pick between generating a message or calling one or more tools.
    // 3. required means the model must call one or more tools.
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see
    /// how to specify which tools the model can call.
    tool_choice: String,
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how
    /// to specify which tools the model can call.
    tools: List(Tool),
    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
    /// each with an associated log probability.
    top_logprobs: Int,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens
    /// with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or temperature but not both.
    /// gpt-5 and o-series models only
    top_p: Float,
    /// The truncation strategy to use for the model response.
    /// 1. auto: If the input to this Response exceeds the model's context window size, the model will truncate the response to fit the
    /// context window by dropping items from the beginning of the conversation.
    /// 2. disabled (default): If the input size will exceed the context window size for a model, the request will fail with a 400 error.
    truncation: String,
    /// Represents token usage details including input tokens, output tokens, a breakdown of output tokens, and the total tokens used.
    usage: Usage,
    /// DEPRECATED
    /// This field is being replaced by safety_identifier and prompt_cache_key. Use prompt_cache_key instead to maintain caching
    /// optimizations. A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests
    /// and to help OpenAI detect and prevent abuse. Learn more.
    user: Option(String),
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the
    /// object in a structured format, and querying for objects via API or the dashboard.
    /// Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
    metadata: Dict(String, String),
  )
}

pub type Billing {
  Billing(payer: String)
}

pub type Error {
  Error(code: String, message: String)
}

pub type IncompleteDetails {
  IncompleteDetails(
    /// The reason why the response is incomplete.
    reason: String,
  )
}

// TODO: add Input Item List
pub type Instructions {
  /// A text input to the model, equivalent to a text input with the developer role.
  InstructionsText(String)
}

// This is the type key
pub type Output {
  Message(
    content: List(OutputContent),
    id: String,
    role: String,
    status: String,
  )
}

pub type OutputContent {
  OutputText(annotations: List(Annotation), text: String)
}

pub type Annotation {
  /// A citation to a file.
  FileCitation(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
  /// A citation for a web resource used to generate a model response.
  URLCitation(
    /// The index of the last character of the URL citation in the message.
    end_index: Int,
    /// The index of the first character of the URL citation in the message.
    start_index: Int,
    /// The title of the web resource.
    title: String,
    /// The URL of the web resource.
    url: String,
  )
  /// A path to a file.
  FilePath(
    /// The ID of the file.
    file_id: String,
    /// The index of the file in the list of files.
    index: Int,
  )
}

pub type Reasoning {
  Reasoning(
    /// Constrains effort on reasoning for reasoning models. Currently supported values are minimal, low, medium,
    /// and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
    effort: Option(String),
    /// A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's
    /// reasoning process. One of auto, concise, or detailed.
    summary: Option(String),
  )
}

pub type Text {
  Text(format: Format, verbosity: String)
}

pub type Format {
  Format(type_: String)
}

pub type Tool {
  /// Defines a function in your own code the model can choose to call. Learn more about [function calling](https://platform.openai.com/docs/guides/function-calling).
  Function(
    /// The name of the function to call.
    name: String,
    /// A JSON schema object describing the parameters of the function.
    /// TODO: Should this be a JSON type?
    parameters: String,
    /// Whether to enforce strict parameter validation.
    strict: Bool,
    /// A description of the function. Used by the model to determine whether or not to call the function.
    description: Option(String),
  )
  // /// A tool that searches for relevant content from uploaded files. Learn more about the [file search tool](https://platform.openai.com/docs/guides/tools-file-search).
  // FileSearch(
  //   /// The IDs of the vector stores to search.
  //   vector_store_ids: List(String),
  //   /// A filter to apply based on file attributes.
  //   filters: FileSearchFilters,
  //   /// The maximum number of results to return. This number should be between 1 and 50 inclusive.
  //   max_num_results: Int,
  //   /// Ranking options for search.
  //   ranking_options: RankingOptions,
  // )
  // ComputerUse(
  //   /// The height of the computer display.
  //   display_height: Int,
  //   /// The width of the computer display.
  //   display_width: Int,
  //   /// The type of computer environment to control.
  //   environment: Environment,
  // )
  // WebSearch(
  //   /// High level guidance for the amount of context window space to use for the search.
  //   search_context_size: SearchContextSize,
  //   /// Approximate location parameters for the search.
  //   user_location: Option(UserLocation),
  // )
}

pub type Usage {
  Usage(
    input_tokens: Int,
    input_tokens_details: InputTokensDetails,
    output_tokens: Int,
    output_tokens_details: OutputTokensDetails,
    total_tokens: Int,
  )
}

pub type InputTokensDetails {
  InputTokensDetails(cached_tokens: Int)
}

pub type OutputTokensDetails {
  OutputTokensDetails(reasoning_tokens: Int)
}
// Response(\"metadata\": {}\n}")
